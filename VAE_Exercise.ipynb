{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VAE_Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sinahmr/vae-ta/blob/master/VAE_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nq5dlNliOvR8"
      },
      "source": [
        "# CE-40959: Deep Learning\n",
        "## HW5 - Variational Autoencoder\n",
        "#### 35 Points | Deadline: 4th of Khordad\n",
        "\n",
        "---\n",
        "\n",
        "Name:\n",
        "\n",
        "Student No.:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_moEmySpQVH",
        "colab_type": "text"
      },
      "source": [
        "**!!! CAUTION !!!**\n",
        "\n",
        "To prevent Colab from disconnecting, insert the following javascript code into the inspector's console.\n",
        "\n",
        "More info: \n",
        "[medium](https://medium.com/@shivamrawat_756/how-to-prevent-google-colab-from-disconnecting-717b88a128c0),\n",
        "[stackoverflow](https://stackoverflow.com/questions/57113226/how-to-prevent-google-colab-from-disconnecting)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "function ClickConnect(){\n",
        "  console.log(\"Working\"); \n",
        "  document\n",
        "    .querySelector(\"#top-toolbar > colab-connect-button\")\n",
        "    .shadowRoot\n",
        "    .querySelector(\"#connect\")\n",
        "    .click()\n",
        "}\n",
        "\n",
        "setInterval(ClickConnect,60000)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMrnZWR81u0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################### Problem 00 ####################\n",
        "# Remember to write your Name and Student No. in the first cell :D\n",
        "####################### End ########################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j4Yuka_u595a",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "abFG0W_D6TnY",
        "colab": {}
      },
      "source": [
        "LOG_INTERVAL = 200\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 5e-5\n",
        "\n",
        "CUDA = True\n",
        "device = torch.device(\"cuda\" if CUDA else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2WNptQFm-UoA",
        "colab": {}
      },
      "source": [
        "# Reproducibility options\n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "if CUDA:\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6u5VGTbrS4E",
        "colab_type": "text"
      },
      "source": [
        "## 1) Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_K3w1YkLHiKF",
        "colab": {}
      },
      "source": [
        "def show(images, rows=1):\n",
        "    \"\"\"\n",
        "    This function gets multiple MNIST images and plots them in the given number\n",
        "    of rows.\n",
        "    \"\"\"\n",
        "    if images.shape[-1] == 784 or images.shape[1] == 1:\n",
        "        images = images.reshape(-1, 28, 28)\n",
        "\n",
        "    cols = np.ceil(images.shape[0] / rows)\n",
        "    plt.rcParams['figure.figsize'] = (cols, rows)\n",
        "    \n",
        "    for i in range(images.shape[0]):\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        plt.imshow(images[i], cmap=\"gray\", vmin=0, vmax=1)\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_iUrebfa-TUc"
      },
      "source": [
        "## 2) Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHuoZubnQxkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get MNIST dataloader\n",
        "\n",
        "data_loader_kwargs = {\n",
        "    'batch_size': BATCH_SIZE, \n",
        "    'shuffle': True,\n",
        "    'pin_memory': True,\n",
        "    'num_workers': 4,\n",
        "}\n",
        "\n",
        "train_dataset = datasets.MNIST('./data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, **data_loader_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, **data_loader_kwargs)\n",
        "\n",
        "print(train_dataset.data.max().item())  # But when using `train_loader` your data is normalized, learn more about transforms.ToTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFH2dkj-q64d",
        "colab_type": "text"
      },
      "source": [
        "## 3) Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz3als6o5XGd",
        "colab_type": "text"
      },
      "source": [
        "### 3.1) Autoencoder (7 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX1-1FLyFWIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This class defines an Autoencoder\n",
        "class AE(nn.Module):\n",
        "    def __init__(self, input_dim, encoder_dims, z_dim, decoder_dims):\n",
        "        super(AE, self).__init__()\n",
        "\n",
        "        self.type_str = 'AE'\n",
        "        self.z_dim = z_dim\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        \n",
        "        # Encoder part\n",
        "        encoder_layers = list()\n",
        "        ############### Problem 01 (2 pts) ################\n",
        "        # Iterate over `encoder_dims` and create fully connected layers\n",
        "        # Use ReLU activation function after each FC layer\n",
        "        # Append all layers to `encoder_layers`\n",
        "        pass\n",
        "        ####################### End ########################\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "        \n",
        "        # Compute Z\n",
        "        self.z_layer = None\n",
        "        ################ Problem 02 (1 pts) ################\n",
        "        # Define a fully connected layer that maps the output of previous part\n",
        "        # to `z_dim` dimensions, store it in `self.z_layer`\n",
        "        # No activation function is needed after this layer\n",
        "        pass\n",
        "        ####################### End ########################\n",
        "\n",
        "        # Decoder part\n",
        "        decoder_layers = list()\n",
        "        ################ Problem 03 (2 pts) ################\n",
        "        # Define the decoder part (Use `decoder_dims`)\n",
        "        # Use ReLU activation function after each FC layer (except the last one)\n",
        "        # The last layer output should be of `input_dim` size\n",
        "        # Append all layers to `decoder_layers`\n",
        "        pass\n",
        "        ####################### End ########################\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_hat, z = None, None\n",
        "        dropped_out = self.dropout(x)\n",
        "        ################ Problem 04 (1 pts) ################\n",
        "        # Do the forward pass\n",
        "        # Compute `x_hat` (reconstructed inputs), and `z` (the latent variables)\n",
        "        pass\n",
        "        ####################### End ########################\n",
        "        return x_hat, z, None  # Last output is returned for the sake of compatibility\n",
        "\n",
        "    def get_loss(self, x, x_hat, *_):\n",
        "        ################ Problem 05 (1 pts) ################\n",
        "        # Compute and return the MSE between x and x_hat\n",
        "        pass\n",
        "        ####################### End ########################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4876bnv5b-8",
        "colab_type": "text"
      },
      "source": [
        "### 3.2) Variational Autoencoder (10 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "InbXIT_NP9uK",
        "colab": {}
      },
      "source": [
        "# This class defines a Variationl Autoencoder\n",
        "# It inherits the AE class\n",
        "class VAE(AE):\n",
        "    def __init__(self, input_dim, encoder_dims, z_dim, decoder_dims):\n",
        "        super(VAE, self).__init__(input_dim, encoder_dims, z_dim, decoder_dims)\n",
        "\n",
        "        self.type_str = 'VAE'\n",
        "        del self.z_layer  # z_layer is not needed anymore\n",
        "\n",
        "        # Drouput, Encoder, and Decoder have been defined in AE class\n",
        "\n",
        "        # mu and sigma_matrix part\n",
        "        self.mu_layer, self.logvar_layer = None, None\n",
        "        ################ Problem 06 (1 pts) ################\n",
        "        # Define mu and logvar layers\n",
        "        # Notice that we should have a logvar_layer, not a sigma_layer\n",
        "        # Do not use any activation function\n",
        "        pass\n",
        "        ####################### End ########################\n",
        "\n",
        "    @staticmethod\n",
        "    def reparameterize(mu, logvar):\n",
        "        z = None\n",
        "        ################ Problem 07 (3 pts) ################\n",
        "        # Sample `z` from N(`mu`, e^`logvar`) in a way that the gradient can\n",
        "        # backpropagate through this sampling operation\n",
        "        pass\n",
        "        ####################### End ########################\n",
        "        return z\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_hat, mu, logvar = None, None, None\n",
        "        dropped_out = self.dropout(x)\n",
        "        ################ Problem 08 (1 pts) ################\n",
        "        # Do the forward pass\n",
        "        # Compute `x_hat` (reconstructed inputs), `mu`, and `logvar` (outputs of\n",
        "        # `mu_layer` and `logvar_layer` respectively)\n",
        "        # Use reparameterization trick (the function you have implemented)\n",
        "        pass\n",
        "        ####################### End ########################\n",
        "        return x_hat, mu, logvar\n",
        "    \n",
        "    def get_loss(self, x, x_hat, mu, logvar):\n",
        "        MSE, KLD = 0, 0\n",
        "        ################ Problem 09 (3 pts) ################\n",
        "        # Compute the VAE loss (Assuming Guassian distribution for the decoder\n",
        "        # output)\n",
        "        pass\n",
        "        ####################### End ########################\n",
        "        return MSE + KLD\n",
        "\n",
        "    def generate(self, n):\n",
        "        samples = None\n",
        "        ################ Problem 10 (2 pts) ################\n",
        "        # Generate `n` random noises from N(0, I), feed it into the decoder and\n",
        "        # generate `n` samples\n",
        "        pass\n",
        "        ####################### End ########################\n",
        "        return samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRowa40urDqt",
        "colab_type": "text"
      },
      "source": [
        "## 4) Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw1qKRctxEy5",
        "colab_type": "text"
      },
      "source": [
        "### 4.1) Required functions (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VR0_6oqA_eoI",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, verbose=True):\n",
        "    \"\"\"\n",
        "    This function trains a `model` on `train_loader` for 1 epoch and prints the\n",
        "    loss value\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for batch_idx, (x, _) in enumerate(tqdm(train_loader, desc='Batches', leave=False)):\n",
        "        x = x.flatten(start_dim=1).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_hat, mu, logvar = model(x)\n",
        "        loss = model.get_loss(x, x_hat, mu, logvar)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if verbose and batch_idx % LOG_INTERVAL == LOG_INTERVAL-1:\n",
        "            print('    Train [%d/%d]\\t | \\tLoss: %.5f' % (batch_idx * x.shape[0], len(train_loader.dataset), loss.item() / x.shape[0]))\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    if verbose:\n",
        "        print('==> Train | Average loss: %.4f' % train_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "el0PNTe8CUoh",
        "colab": {}
      },
      "source": [
        "def test(model, verbose=True):\n",
        "    \"\"\"\n",
        "    This function tests a `model` on a `test_loader` and prints the loss value\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, _ in test_loader:\n",
        "            x = x.flatten(start_dim=1).to(device)\n",
        "            \n",
        "            x_hat, mu, logvar = model(x)\n",
        "            loss = model.get_loss(x, x_hat, mu, logvar)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    if verbose:\n",
        "        print('==> Test  | Average loss: %.4f' % test_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PPQtOVFm-7Ng",
        "colab": {}
      },
      "source": [
        "def run(model, n_epoch, verbose=True):\n",
        "    \"\"\"\n",
        "    This function will optimize parameters of `model` for `n_epoch` epochs\n",
        "    on `train_loader` and validate it on `test_loader`.\n",
        "    \"\"\"\n",
        "    ################ Problem 11 (1 pts) ################\n",
        "    # Send `model` to the desired device, defined in `device`\n",
        "    pass\n",
        "    ####################### End ########################\n",
        "\n",
        "    optimizer = None\n",
        "    ################ Problem 12 (1 pts) ################\n",
        "    # Initialize a new Adam optimizer\n",
        "    pass\n",
        "    ####################### End ########################\n",
        "\n",
        "    for epoch in trange(1, n_epoch+1, desc='Epochs', leave=True):\n",
        "        if verbose:\n",
        "            print('\\nEpoch %d:' % epoch)\n",
        "        train(model, optimizer, verbose)\n",
        "        test(model, verbose)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G5CNE3UxJSt",
        "colab_type": "text"
      },
      "source": [
        "### 4.2) Do run (1 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FJ3NdKDRC0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ae_low_dim, vae_low_dim, ae_high_dim, vae_high_dim = 4 * [None]\n",
        "################ Problem 13 (1 pts) ################\n",
        "# Define 4 models, with these specifications:\n",
        "# name           z_dim   encoder_dims    decoder_dims    training_n_epoch\n",
        "# ae_low_dim     2       3 layers        3 layers        a\n",
        "# vae_low_dim    2       same as above   same as above   a\n",
        "# ae_high_dim    >= 20   3 layers        3 layers        b\n",
        "# vae_high_dim   >= 20   same as above   same as above   b\n",
        "# `a`, `b`, `encoder_dims`, and `decoder_dims` of your choice\n",
        "# Then train all of 4 models\n",
        "pass\n",
        "####################### End ########################\n",
        "\n",
        "models = [ae_low_dim, vae_low_dim, ae_high_dim, vae_high_dim]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfzycmf0qb3f",
        "colab_type": "text"
      },
      "source": [
        "## 5) Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKPnUT59pppK",
        "colab_type": "text"
      },
      "source": [
        "### 5.1) Representation (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x8McHCLZ7MS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################ Problem 14 (2 pts) ################\n",
        "# Run the current cell,\n",
        "# Then answer the following cell's question in the cell itself\n",
        "####################### End ########################\n",
        "\n",
        "def visualize_2d(model, n_batch):\n",
        "    assert model.z_dim == 2\n",
        "    model.eval()\n",
        "\n",
        "    n = BATCH_SIZE * n_batch\n",
        "    Y, Z = torch.zeros(n, dtype=int), torch.zeros((n, model.z_dim))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x, y) in enumerate(test_loader):\n",
        "            if batch_idx >= n_batch:\n",
        "                break\n",
        "            current_range = range(batch_idx * BATCH_SIZE, (batch_idx+1) * BATCH_SIZE)\n",
        "            Y[current_range] = y\n",
        "            x = x.flatten(start_dim=1).to(device)\n",
        "            _, z, _ = model(x)\n",
        "            Z[current_range] = z.cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    scatter = ax.scatter(Z[:, 0], Z[:, 1], c=Y, cmap='tab10')\n",
        "    legend = ax.legend(*scatter.legend_elements(), bbox_to_anchor=(1.04, 1), title=\"label\")\n",
        "    ax.add_artist(legend)\n",
        "    plt.show()\n",
        "\n",
        "print('AE:')\n",
        "visualize_2d(ae_low_dim, 10)\n",
        "\n",
        "print('\\nVAE:')\n",
        "visualize_2d(vae_low_dim, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkooNpyx7g-W",
        "colab_type": "text"
      },
      "source": [
        "#### Question\n",
        "\n",
        "<div dir=\"rtl\"> \n",
        "دو مدل\n",
        "<tt>ae_low_dim</tt> \n",
        "و \n",
        "<tt>vae_low_dim</tt> \n",
        "(که هر دوی آن‌ها فضای نهانی به بعد ۲ دارند)\n",
        "را در نظر بگیرید.\n",
        "وقتی داده‌ای را به هر یک از این دو مدل ورودی می‌دهیم، هر کدام از آن‌ها پس از انجام عملیات مربوط به بخش\n",
        "Encoder\n",
        "خود، توزیع دومتغیره‌ای از نوع گاوسی روی فضای نهان ایجاد می‌کنند. برای تعدادی داده از مجموعه‌ی داده‌های تست میانگین این توزیع‌ها را روی یک نمودار رسم کردیم\n",
        "(برای هر کدام از دو مدل نامبرده به صورت جداگانه).\n",
        "در واقع هر کدام از این دو نمودار، نمودار بازنمایی در فضای نهان برای تعدادی داده است. رنگ هر نقطه نیز برچسب داده‌ی مربوط به هر بازنمایی را مشخص می‌کند.\n",
        "\n",
        "الف) \n",
        "به نظر شما کدام مدل فضای نهان بهتری را یاد گرفته‌است؟\n",
        "(با ذکر دلیل)\n",
        "\n",
        "ب)\n",
        "پیشبینی می‌کنید تصاویر میانی در\n",
        "interpolation\n",
        "بین دو تصویر، در کدام یک از این دو مدل تصاویر معنی‌دارتری هستند؟ چرا؟\n",
        "(اگر با\n",
        "interpolation\n",
        "آشنایی ندارید، ابتدا آخرین بخش این\n",
        "notebook\n",
        "را انجام دهید.)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<b>پاسخ خود را در دو قسمت زیر بنویسید:</b>\n",
        "\n",
        "الف)\n",
        "...\n",
        "\n",
        "ب)\n",
        "...\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C80cNeAqpwxY",
        "colab_type": "text"
      },
      "source": [
        "### 5.2) Reconstruction (3 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4E6wTAyvBcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_reconstructions(models, n):\n",
        "    x = next(iter(test_loader))[0][:n]  # Get a batch and choose `n` of images\n",
        "    print('Data')\n",
        "    show(x.squeeze(1))\n",
        "\n",
        "    x = x.flatten(start_dim=1).to(device)\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "        x_hat, _, _ = model(x)\n",
        "        print('%s %dD' % (model.type_str, model.z_dim))\n",
        "        show(x_hat.detach().cpu())\n",
        "\n",
        "plot_reconstructions(models, 20)\n",
        "\n",
        "################ Problem 15 (3 pts) ################\n",
        "# Just run this cell, make sure the output is saved in the uploaded notebook\n",
        "####################### End ########################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKcmKLyPp7rT",
        "colab_type": "text"
      },
      "source": [
        "### 5.3) Generation (3 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hotmmuht2Mnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('VAE %dD' % vae_high_dim.z_dim)\n",
        "generated_imgs = vae_high_dim.generate(60)\n",
        "show(generated_imgs.detach().cpu(), 3)\n",
        "\n",
        "################ Problem 16 (3 pts) ################\n",
        "# Just run this cell, make sure the output is saved in the uploaded notebook\n",
        "####################### End ########################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUHevpnQuwq3",
        "colab_type": "text"
      },
      "source": [
        "### 5.4) Interpolation (7 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9qGDX6uu05D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_interpolations(models, n_rows, n_cols=10):\n",
        "    \"\"\"\n",
        "    This function interpolates n_cols images between two random MNIST image\n",
        "    \"\"\"\n",
        "    plt.rcParams['figure.figsize'] = (n_cols, n_rows)\n",
        "    \n",
        "    x = next(iter(test_loader))[0][:2 * n_rows].flatten(start_dim=1).to(device)\n",
        "    for i in range(n_rows):\n",
        "        img1 = x[2 * i]\n",
        "        img2 = x[2 * i + 1]\n",
        "        \n",
        "        for model in models:\n",
        "            model.eval()\n",
        "            images = None\n",
        "            ################ Problem 17 (5 pts) ################\n",
        "            # Find the representations of `img1, img2` in the latent space and call them `z1, z2`\n",
        "            # Interpolate `n_cols` tensors (evenly spaced) between `z1` and `z2` (including both)\n",
        "            # Feed these tensors to the decoder and get the resulting reconstruced images\n",
        "            # Store these `n_cols` images in `images[1:-1]`\n",
        "            # Set `images[0], images[-1]` to `img1, img2` respectively\n",
        "            # Prepare `images` tensor to be passed into `show` function\n",
        "            pass\n",
        "            ####################### End ########################\n",
        "            print('%s %dD' % (model.type_str, model.z_dim))\n",
        "            show(images)\n",
        "        print('---\\n')\n",
        "\n",
        "plot_interpolations(models, 5)\n",
        "\n",
        "################ Problem 18 (2 pts) ################\n",
        "# Run this cell, make sure the output is saved in the uploaded notebook\n",
        "####################### End ########################"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}